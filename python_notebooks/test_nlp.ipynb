{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests For Natural Language Processing\n",
    "\n",
    "This NoteBook is used to test the Spark-NLP for processing text\n",
    "\n",
    "## Initialize Spark-NLP\n",
    "To initialize Spark-NLP it is neccesary to import the library and use the method start().\n",
    "\n",
    "For Spark-NLP to work it is also neccessary to install the libary pyspark using pip is not installed using the requirements.txt file.\n",
    "\n",
    "```sh\n",
    "> pip install pyspark==3.3.1\n",
    "``` \n",
    "\n",
    "It is also neccesary to install Java. For example in Ubuntu:\n",
    "\n",
    "```sh\n",
    "> sudo apt install openjdk-17-jdk-headless\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:21:29 WARN Utils: Your hostname, user-Pc resolves to a loopback address: 127.0.1.1; using 192.168.1.143 instead (on interface enp37s0)\n",
      "24/09/30 00:21:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/usernormal/projects/tfm/final/Net-Analyzer-SNA-NLP-Analysisi-/python_notebooks/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/usernormal/.ivy2/cache\n",
      "The jars for the packages stored in: /home/usernormal/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c4d49cab-47f5-47f1-9784-b086d756cc68;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.4.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.18.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 1123ms :: artifacts dl 37ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.4.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.18.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   84  |   0   |   0   |   5   ||   79  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c4d49cab-47f5-47f1-9784-b086d756cc68\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 79 already retrieved (0kB/19ms)\n",
      "24/09/30 00:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/30 00:21:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5.4.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check pretained models available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Pipeline\n",
    "The following code shows hot to use a pretrained pipeline.\n",
    "This example has been obtained from sparknlp.org site. \n",
    "\n",
    "In this case a pretained model is used to perform a series of NLP task in a given text, generally before processing the text.\n",
    "For example we would preproccess - annotate - the text before trying to classificate or translate it with a model - or before using it for trainning a model -. Ofcourse, more steps would be needed in some cases so the input of the model can proccess the information, like converting the \"words\"/tokens to a numeric value - this is called [word embedding](https://www.turing.com/kb/guide-on-word-embeddings-in-nlp) -.\n",
    "\n",
    "Some of these task are:\n",
    "\n",
    "\n",
    "- Token: divides the sentences in tokens, generally it correspond to each word separated by a space, minus the puntuation. In words like \"don't\", the correct way in which the token is divided is \"do\" and \"n't\", so it is easier for the algorithms to undertand that is a negative no.\n",
    "\n",
    "- lemmatization: it simplifies the tokens to get common forms from inflections. For example, orginze, organizes and organizing will be converted to orginize.\n",
    "\n",
    "- pos: determines the type of token, and gives a tag to each type. For example: JJ adjetive, NNP proper name singular, VBP Verb, non-3rd person singular present... [TAGS](https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html)\n",
    "\n",
    "- Stemming: is similar to lemmatization, but instead of looking for a base form of a work, it will just chop the end of some words. For example, if will have Finally and Final, the common used token will be Fina.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:21:45 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "24/09/30 00:21:45 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approximate size to download 9 MB\n",
      "Download done! Loading the resource.\n",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "document: [\"We are very happy about SparkNLP. But We don't know how to use it yet.\"]\n",
      "spell: [\"We\", \"are\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "pos: [\"PRP\", \"VBP\", \"RB\", \"JJ\", \"IN\", \"NNP\", \".\", \"CC\", \"PRP\", \"VBP\", \"VB\", \"WRB\", \"TO\", \"VB\", \"PRP\", \"RB\", \".\"]\n",
      "lemmas: [\"We\", \"be\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "token: [\"We\", \"are\", \"very\", \"happy\", \"about\", \"SparkNLP\", \".\", \"But\", \"We\", \"don't\", \"know\", \"how\", \"to\", \"use\", \"it\", \"yet\", \".\"]\n",
      "stems: [\"we\", \"ar\", \"veri\", \"happi\", \"about\", \"sparknlp\", \".\", \"but\", \"we\", \"don't\", \"know\", \"how\", \"to\", \"us\", \"it\", \"yet\", \".\"]\n",
      "sentence: [\"We are very happy about SparkNLP.\", \"But We don't know how to use it yet.\"]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import json\n",
    "\n",
    "explain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n",
    "annotations = explain_document_pipeline.annotate(\"We are very happy about SparkNLP. But We don't know how to use it yet.\")\n",
    "\n",
    "for key, array in annotations.items():\n",
    "    formatted_array = ', '.join(json.dumps(item) for item in array)\n",
    "    print(f\"{key}: [{formatted_array}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text Sentiment\n",
    "The following code shows how to use a pretained spark-nlp model to classify text.\n",
    "\n",
    "In this case the classifier will check for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "analyze_sentiment download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:22:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx size to download 4,8 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:22:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "24/09/30 00:22:02 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_sentiment download started this may take some time.\n",
      "Approximate size to download 4,8 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "|id |text               |result    |\n",
      "+---+-------------------+----------+\n",
      "|1  |I hate you         |[negative]|\n",
      "|2  |I love this product|[positive]|\n",
      "|3  |You are an idiot   |[positive]|\n",
      "|4  |This is amazing    |[positive]|\n",
      "+---+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# Example using a pre-trained sentiment analysis pipeline\n",
    "# Replace with a relevant pipeline if available\n",
    "pipeline = PretrainedPipeline(\"analyze_sentiment\", lang=\"en\")\n",
    "\n",
    "# Sample data\n",
    "data = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"I love this product\"),\n",
    "    (3, \"You are an idiot\"),\n",
    "    (4, \"This is amazing\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Apply pipeline\n",
    "result = pipeline.transform(data)\n",
    "\n",
    "result.select(\"id\", \"text\", \"sentiment.result\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Text Hate Speech with pretrained models\n",
    "The following code shows how to use a pretained spark-nlp model to classify text, while a like shows the line fit, this does not train the modle.\n",
    "\n",
    "In this case the classifier will check for hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_sequence_classifier_dehatebert_mono download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:26:32 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 599 MB\n",
      "[OK!]\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|id |text                                                                                                                                            |class                                                                                          |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "|1  |I hate you                                                                                                                                      |[{category, 0, 9, NON_HATE, {sentence -> 0, NON_HATE -> 0.58358485, HATE -> 0.41641515}, []}]  |\n",
      "|2  |You are an amazing person.                                                                                                                      |[{category, 0, 25, NON_HATE, {sentence -> 0, NON_HATE -> 0.97737086, HATE -> 0.022629123}, []}]|\n",
      "|3  |This is terrible.                                                                                                                               |[{category, 0, 16, NON_HATE, {sentence -> 0, NON_HATE -> 0.9723487, HATE -> 0.02765133}, []}]  |\n",
      "|4  |What a wonderful day!                                                                                                                           |[{category, 0, 20, NON_HATE, {sentence -> 0, NON_HATE -> 0.9770796, HATE -> 0.02292045}, []}]  |\n",
      "|5  |You are a moron.                                                                                                                                |[{category, 0, 15, NON_HATE, {sentence -> 0, NON_HATE -> 0.8094599, HATE -> 0.19054009}, []}]  |\n",
      "|6  |Nazis are cool                                                                                                                                  |[{category, 0, 13, NON_HATE, {sentence -> 0, NON_HATE -> 0.9745119, HATE -> 0.025488053}, []}] |\n",
      "|7  |Nazis are not cool                                                                                                                              |[{category, 0, 17, NON_HATE, {sentence -> 0, NON_HATE -> 0.9746483, HATE -> 0.025351731}, []}] |\n",
      "|8  |RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corr…|[{category, 0, 139, HATE, {sentence -> 0, NON_HATE -> 0.09545476, HATE -> 0.90454525}, []}]    |\n",
      "+---+------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, BertForSequenceClassification\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 0. Configure the step for document assembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    ".setInputCol('text') \\\n",
    ".setOutputCol('document')\n",
    "\n",
    "# 1. Configure the step that will get tokens from text\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "\n",
    "# 2. Configure the step that will classify the text\n",
    "sequenceClassifier = BertForSequenceClassification \\\n",
    "    .pretrained('bert_sequence_classifier_dehatebert_mono', 'en') \\\n",
    "    .setInputCols(['token', 'document']) \\\n",
    "    .setOutputCol('class') \\\n",
    "    .setCaseSensitive(True) \\\n",
    "    .setMaxSentenceLength(512)\n",
    "\n",
    "# 3. Configure the step that will obtain the result from the proccess\n",
    "#finisher = Finisher() \\\n",
    "#    .setInputCols([\"class\"]) \\\n",
    "#    .setOutputCols([\"prediction\"]) \\\n",
    "#    .setCleanAnnotations(False)\n",
    "\n",
    "# 4. Organize the steps\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    sequenceClassifier\n",
    "    #finisher\n",
    "])\n",
    "\n",
    "# 5. Classify sample data\n",
    "# Sample data\n",
    "example = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"You are an amazing person.\"),\n",
    "    (3, \"This is terrible.\"),\n",
    "    (4, \"What a wonderful day!\"),\n",
    "    (5, \"You are a moron.\"), \n",
    "    (6,\"Nazis are cool\"), \n",
    "    (7,\"Nazis are not cool\"),\n",
    "    (8,\"RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corr…\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# If we want to modify the model\n",
    "#model = pipeline.fit(example)\n",
    "\n",
    "result = pipeline.fit(example).transform(example)\n",
    "result.select(\"id\", \"text\", \"class\").show(truncate=False)\n",
    "#result.select(\"id\", \"text\", \"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_classifier_bert_base_uncased_hatexplain download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 00:27:01 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 390,4 MB\n",
      "[OK!]\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                             |class                                                                                                                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "|I hate you                                                                                                                                       |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.13802324, normal -> 0.5470316, offensive -> 0.3149452}, []}]         |\n",
      "|You are an amazing person.                                                                                                                       |[{category, 0, 25, normal, {sentence -> 0, hate speech -> 0.060175754, normal -> 0.64119154, offensive -> 0.29863268}, []}]     |\n",
      "|This is terrible.                                                                                                                                |[{category, 0, 16, normal, {sentence -> 0, hate speech -> 0.07880838, normal -> 0.62920123, offensive -> 0.2919904}, []}]       |\n",
      "|What a wonderful day!                                                                                                                            |[{category, 0, 20, normal, {sentence -> 0, hate speech -> 0.06310587, normal -> 0.6823274, offensive -> 0.25456673}, []}]       |\n",
      "|You are a moron.                                                                                                                                 |[{category, 0, 15, offensive, {sentence -> 0, hate speech -> 0.051471725, normal -> 0.3786545, offensive -> 0.56987375}, []}]   |\n",
      "|Nazis are cool                                                                                                                                   |[{category, 0, 13, offensive, {sentence -> 0, hate speech -> 0.12458121, normal -> 0.36013916, offensive -> 0.5152796}, []}]    |\n",
      "|RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corr…|[{category, 0, 140, hate speech, {sentence -> 0, hate speech -> 0.76267284, normal -> 0.09264287, offensive -> 0.14468427}, []}]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|text              |class                                                                                                                       |\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|I hate you        |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.13802324, normal -> 0.5470316, offensive -> 0.3149452}, []}]     |\n",
      "|I love you        |[{category, 0, 9, normal, {sentence -> 0, hate speech -> 0.08458458, normal -> 0.66519564, offensive -> 0.25021976}, []}]   |\n",
      "|Nazis are cool    |[{category, 0, 13, offensive, {sentence -> 0, hate speech -> 0.12458121, normal -> 0.36013916, offensive -> 0.5152796}, []}]|\n",
      "|Nazis are not cool|[{category, 0, 17, offensive, {sentence -> 0, hate speech -> 0.13478369, normal -> 0.31091493, offensive -> 0.5543014}, []}]|\n",
      "+------------------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, BertForSequenceClassification\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "seq_classifier = BertForSequenceClassification.pretrained(\"bert_classifier_bert_base_uncased_hatexplain\",\"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"class\")\n",
    "\n",
    "pipeline = Pipeline(stages=[documentAssembler, tokenizer, seq_classifier])\n",
    "\n",
    "example = spark.createDataFrame([\n",
    "    (1, \"I hate you\"),\n",
    "    (2, \"You are an amazing person.\"),\n",
    "    (3, \"This is terrible.\"),\n",
    "    (4, \"What a wonderful day!\"),\n",
    "    (5, \"You are a moron.\"), \n",
    "    (6,\"Nazis are cool\"), \n",
    "    (7,\"RT @TheBeardedOak: R.I.P *****.\\r\\nDied first on May 16th 1943 when tragically hit by a car.\\r\\nDied second in July 2020 due to Political Corr…\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "result = pipeline.fit(example).transform(example)\n",
    "result.select(\"text\", \"class\").show(truncate=False)\n",
    "\n",
    "data = spark.createDataFrame(\n",
    "    [(1,\"I hate you\"), (2,\"I love you\"), (3,\"Nazis are cool\"), (4,\"Nazis are not cool\")], \n",
    "    [\"id\",\"text\"]\n",
    ")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.select(\"text\", \"class\").show(truncate=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
